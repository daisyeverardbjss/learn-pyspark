## Create account

Complete the sign up process at https://www.databricks.com/try-databricks#account
Chose the Community edition when signing up
using external storage locations

## Setup Cluster

A cluster is a set of compute resources which will run your code

Once you are in the databricks environment, navigate to the `Compute` page on the left and click `Create Compute`

Select the runtime `13.3` LTS from the dropdown then create the compute.
It will take a few minutes to provision your resources

## Upload Databricks Notebooks

To run the notebooks in this repository you will need to upload them to databricks
Navigate to the `Workspace` page on the left then go to your `Home`
Click the 3 dots in the top right and select `import`
This will allow you to drag and drop or navigate to the file you want to upload. Please choose the file located in this repo in `Databricks/Pyspark Intro - Task 1.py`

## Upload CSV file

To upload the data we are using you will need the `Catalog`

Select `Create Table` and leave the `Target Directory` black
Select `Upload file` and `Create Table with UI`
Choose the cluster you create earlier and give your table a name. The name used in the example was `task1_input`
Tick the boxes for the first row to be the header, and to infer the schema, then create the table.

## Run your notebook

Go back to the notebook we uploaded to your Home workspace and click `Connect` in the top right to attach the notebook to a cluster. Chose the cluster you created and wait for this to start. You can then follow the instructions in the notebook to process your data
